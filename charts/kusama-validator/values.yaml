# Validator identity
validatorName: ""
stashAccount: ""
controllerAccount: ""

# Chain configuration
chain: kusama
# Options: kusama, polkadot, westend (testnet)

# Polkadot binary version
# Check releases: https://github.com/paritytech/polkadot-sdk/releases
image:
  repository: parity/polkadot
  # Pin to a specific version for production!
  # Examples: v1.7.0, stable, latest
  tag: stable
  pullPolicy: IfNotPresent

# Node configuration
node:
  # Validator name shown on telemetry
  name: ""
  # Enable validator mode
  validator: true
  # RPC settings
  # Note: 'unsafe' is required for key rotation (author_rotateKeys).
  # Security is handled via ClusterIP (internal only) and Firewall (blocked from internet).
  rpcMethods: unsafe
  # Telemetry
  telemetryUrl: "wss://telemetry.polkadot.io/submit/ 0"

# Sync configuration - speeds up initial sync dramatically
sync:
  # Sync mode: full, fast, warp
  # - warp: Fastest! Downloads finality proofs (~10-15 minutes)
  # - fast: Downloads recent blocks (~1-2 hours)
  # - full: Downloads all blocks (slowest, days)
  mode: warp

  # Database snapshot restore (optional, for even faster startup)
  # Recommended for production deployments to minimize downtime
  snapshot:
    enabled: false
    
    # Snapshot URL - choose a provider based on your location
    # Kusama snapshots:
    #   - Polkashots (Global CDN): https://ksm-rocksdb.polkashots.io/snapshot
    #   - Stakeworld (EU): https://snapshots.stakeworld.io/kusama/kusama-latest.tar.lz4
    # Polkadot snapshots:
    #   - Polkashots: https://dot-rocksdb.polkashots.io/snapshot
    # Westend snapshots:
    #   - Polkashots: https://wnd-rocksdb.polkashots.io/snapshot
    url: ""
    
    # Compression format: lz4, zstd, or gz
    # - lz4: Fastest decompression (recommended)
    # - zstd: Good balance of speed and size
    # - gz: Slowest but most compatible
    compression: lz4

# Resource allocation
resources:
  requests:
    cpu: "500m"
    memory: "2Gi"
  limits:
    cpu: "8"
    memory: "16Gi"

# Persistent storage for chain data
persistence:
  enabled: true
  size: 50Gi
  storageClass: "" # Use default

# Service configuration
service:
  type: ClusterIP
  p2pPort: 30333
  rpcPort: 9933
  prometheusPort: 9615

# P2P exposure configuration
# When haproxy.enabled=true, P2P goes through HAProxy
# When haproxy.enabled=false, uses NodePort directly (legacy)
p2pNodePort:
  enabled: false  # Disabled when using HAProxy
  port: 30333

# HAProxy reverse proxy for P2P traffic
haproxy:
  enabled: true
  image:
    repository: haproxy
    tag: "2.9-alpine"
    pullPolicy: IfNotPresent
  # Rate limiting
  rateLimit:
    # Max new connections per second per IP
    connectionsPerSecond: 10
    # Max concurrent connections per IP
    maxConnectionsPerIP: 50
    # Total max connections
    maxConnections: 2000
  # TCP health check interval (ms)
  healthCheckInterval: 5000
  # Resources for HAProxy pods
  resources:
    requests:
      cpu: "50m"
      memory: "64Mi"
    limits:
      cpu: "200m"
      memory: "128Mi"
  # Expose via hostPort on each node
  hostPort: 30333

# Session keys (managed via Sealed Secrets)
sessionKeys:
  # If true, generate keys on startup
  autoGenerate: true
  # Secret name containing pre-generated keys
  existingSecret: ""

# Pod scheduling
affinity:
  # Spread validators across nodes
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: kusama-validator
        topologyKey: kubernetes.io/hostname

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Prometheus monitoring
metrics:
  enabled: true
  serviceMonitor:
    enabled: false # Enable if using Prometheus Operator

# Liveness/Readiness probes
probes:
  liveness:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 30
  readiness:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
